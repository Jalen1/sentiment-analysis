{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 4: Sentiment Analysis - Task 2\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names \n",
    "----\n",
    "Names: __Jalen Wu, Jonathan Zhang__ (Write these in every notebook you submit.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Train a Naive Bayes Model (30 points)\n",
    "----\n",
    "\n",
    "Using `nltk`'s `NaiveBayesClassifier` class, train a Naive Bayes classifier using a Bag of Words as features.\n",
    "\n",
    "Learn more about Naive Bayes here: https://www.nltk.org/_modules/nltk/classify/naivebayes.html \n",
    "\n",
    "Naive Bayes classifiers use Bayesâ€™ theorem for predictions. Naive Bayes can be a good baseline for NLP applications in particular. You can use it as a baseline for your project!\n",
    "\n",
    "**\n",
    "\n",
    "**10 points in Task 5 will be allocated for all 9 graphs (including the one generated here in Task 4 for Naive Bayes Classifier) being:**\n",
    "- Legible\n",
    "- Present below\n",
    "- Properly labeled\n",
    "     - x and y axes labeled\n",
    "     - Legend for accuracy measures plotted\n",
    "     - Plot Title with which model and run number the graph represents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jalenwu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# our utility functions\n",
    "# RESTART your jupyter notebook kernel if you make changes to this file\n",
    "import sentiment_utils as sutils\n",
    "\n",
    "# nltk for Naive Bayes and metrics\n",
    "import nltk\n",
    "import nltk.classify.util\n",
    "from nltk.metrics.scores import (precision, recall, f_measure, accuracy)\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# some potentially helpful data structures from collections\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# so that we can make plots\n",
    "import matplotlib.pyplot as plt\n",
    "# if you want to use seaborn to make plots\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants for the files we are using\n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "DEV_FILE = \"movie_reviews_dev.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in your data and make sure you understand the format\n",
    "# Do not print out too much so as to impede readability of your notebook\n",
    "\n",
    "# Generates a list of tuples from each file. Each tuple is a tokenized sentence.\n",
    "train_tups = sutils.generate_tuples_from_file(TRAIN_FILE)\n",
    "dev_tups = sutils.generate_tuples_from_file(DEV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print((train_tups[1][1]))\n",
    "# print(dev_tups[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a sentiment classifier using NLTK's NaiveBayesClassifier and \n",
    "# a bag of words as features\n",
    "# take a look at the function in lecture notebook 7 (feel free to copy + paste that function)\n",
    "# the nltk classifier expects a dictionary of features as input where the key is the feature name\n",
    "# and the value is the feature value\n",
    "\n",
    "# TODO: Key: Feature Name, Value: Feature Value\n",
    "\n",
    "# need to return a dict to work with the NLTK classifier\n",
    "# Possible problem for students: evaluate the difference \n",
    "# between using binarized features and using counts (non binarized features)\n",
    "\n",
    "# 0 = bad, 1 = good\n",
    "\n",
    "# train_tups[0][0] is a sentence in the training set\n",
    "# train_tups[1][0] is the sentiment of the sentence in the training set (0 bad 1 good)\n",
    "\n",
    "# TODO: 1. what is the dictionary supposed to have as keys and values? keys are words and values are sentiment? But then what if the same word is in sentences that are both good and bad?\n",
    "# 2. how to pass in the dictionary to the classifier? What would be arg1 and arg2?\n",
    "\n",
    "# (__init__(label_probdist, feature_probdist)[source])) <- constructor for NaiveBayesClassifier\n",
    "\n",
    "# list(tuple(dictionary{words : True}, label (0 or 1))) for each example in the training set\n",
    "\n",
    "# TODO: Implement word_feats function\n",
    "def word_feats(tuple_list) -> dict:    \n",
    "    \"\"\"\n",
    "    This function converts a list of words so that they are featurized\n",
    "    for nltk's format for bag-of-words.\n",
    "    \"\"\"\n",
    "    return dict([(word, True) for word in tuple_list])\n",
    "\n",
    "\n",
    "# set up & train a sentiment classifier using NLTK's NaiveBayesClassifier and\n",
    "# classify the first example in the dev set as an example\n",
    "# make sure your output is well-labeled\n",
    "\n",
    "# list(tuple(dictionary{words : True}, label (0 or 1))) for each example in the training set\n",
    "\n",
    "train_list = list()\n",
    "\n",
    "for i in range(len(train_tups[0])):\n",
    "    feats = word_feats(train_tups[0][i])\n",
    "    sentiment = train_tups[1][i]\n",
    "    temp = tuple((feats, sentiment))\n",
    "    train_list.append(temp)\n",
    "\n",
    "nbc = NaiveBayesClassifier.train(train_list)\n",
    "\n",
    "# test to make sure that you can train the classifier and use it to classify a new example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'movie', \"'Gung\", 'Ho', '!', \"'\", ':', 'The', 'Story', 'of', 'Carlson', \"'s\", 'Makin', 'Island', 'Raiders', 'was', 'made', 'in', '1943', 'with', 'a', 'view', 'to', 'go', 'up', 'the', 'moral', 'of', 'American', 'people', 'at', 'the', 'duration', 'of', 'second', 'world', 'war', '.', 'It', 'shows', 'with', 'the', 'better', 'way', 'that', 'the', 'cinema', 'can', 'constitute', 'body', 'of', 'propaganda', '.', 'The', 'value', 'of', 'this', 'film', 'is', 'only', 'collection', 'and', 'no', 'artistic', '.', 'In', 'a', 'film', 'of', 'propaganda', 'it', 'is', 'useless', 'to', 'judge', 'direction', 'and', 'actors', '.', 'Watch', 'that', 'movie', 'if', 'you', 'are', 'interested', 'to', 'learn', 'how', 'propaganda', 'functions', 'in', 'the', 'movies', 'or', 'if', 'you', 'are', 'a', 'big', 'fun', 'of', 'Robert', 'Mitchum', 'who', 'has', 'a', 'small', 'role', 'in', 'the', 'film', '.', 'If', 'you', 'want', 'to', 'see', 'a', 'film', 'for', 'the', 'second', 'world', 'war', ',', 'they', 'exist', 'much', 'better', 'and', 'objective', '.', 'I', 'rated', 'it', '4/10', '.']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(dev_tups[0][0])\n",
    "dev_tups_dict = word_feats(dev_tups[0][0])\n",
    "\n",
    "output = nbc.classify(dev_tups_dict)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the provided dev set, evaluate your model with precision, recall, and f1 score as well as accuracy\n",
    "# You may use nltk's implemented `precision`, `recall`, `f_measure`, and `accuracy` functions\n",
    "# (make sure to look at the documentation for these functions!)\n",
    "# you will be creating a similar graph for logistic regression and neural nets, so make sure\n",
    "# you use functions wisely so that you do not have excessive repeated code\n",
    "# write any helper functions you need in sentiment_utils.py (functions that you'll use in your other notebooks as well)\n",
    "\n",
    "\n",
    "# create a graph of your classifier's performance on the dev set as a function of the amount of training data\n",
    "# the x-axis should be the amount of training data (as a percentage of the total training data)\n",
    "# NOTE : make sure one of your experiments uses 10% of the data, you will need this to answer the first question in task 5\n",
    "# the y-axis should be the performance of the classifier on the dev set\n",
    "# the graph should have 4 lines, one for each of precision, recall, f1, and accuracy\n",
    "# the graph should have a legend, title, and axis labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your model using both a __binarized__ (bag of words representation where we put 1 [true] if the word is there and 0 [false] otherwise) and a __multinomial__ (bag of words representation where we put the count of the word if the word occurs, and 0 otherwise). Use whichever one gives you a better final f1 score on the dev set to produce your graphs.\n",
    "\n",
    "- f1 score binarized: __YOUR ANSWER HERE__\n",
    "- f1 score multinomial: __YOUR ANSWER HERE__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
