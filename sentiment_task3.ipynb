{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 4: Sentiment Analysis - Task 3\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names\n",
    "----\n",
    "Names: __Jalen Wu, Jonathan Zhang__ (Write these in every notebook you submit.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Train a Logistic Regression Model (20 points)\n",
    "----\n",
    "\n",
    "Using `sklearn`'s implementation of `LogisticRegression`, conduct a similar analysis on the performance of a Logistic Regression classifier on the provided data set.\n",
    "\n",
    "Using the `time` module, you'll compare and contrast how long it takes your home-grown BoW vectorizing function vs. `sklearn`'s `CountVectorizer`.\n",
    "\n",
    "Logistic regression is used for binary classification, but can be extended for multi-class classification\n",
    "\n",
    "Read more about logistic regression here - https://www.analyticsvidhya.com/blog/2021/08/conceptual-understanding-of-logistic-regression-for-data-science-beginners/.\n",
    "\n",
    "Recall from task 2 what binarized and multinomial mean here: a __binarized__ bag of words representation is one where we put 1 [true] if the word is there and 0 [false] otherwise, and a __multinomial__ bag of words representation is one where we put the count of the word if the word occurs, and 0 otherwise.\n",
    "\n",
    "**10 points in Task 5 will be allocated for all 9 graphs (including the one generated here in Task 3 for Logistic Regression) being:**\n",
    "- Legible\n",
    "- Present below\n",
    "- Properly labeled\n",
    "     - x and y axes labeled\n",
    "     - Legend for accuracy measures plotted\n",
    "     - Plot Title with which model and run number the graph represents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wendy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from collections import Counter\n",
    "import time\n",
    "import sentiment_utils as sutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Van', 'Dien', 'must', 'cringe', 'with', 'embarrassment', 'at', 'the', 'memory', 'of', 'this', 'ludicrously', 'poor', 'film', ',', 'as', 'indeed', 'must', 'every', 'single', 'individual', 'involved', '.', 'To', 'be', 'honest', 'I', 'am', 'rather', 'embarrassed', 'to', 'admit', 'I', 'watched', 'it', 'from', 'start', 'to', 'finish', '.', 'Production', 'values', 'are', 'somewhere', 'between', 'the', 'original', 'series', 'of', \"'Crossroads\", \"'\", 'and', \"'Prisoner\", 'Cell', 'Block', 'H', \"'\", '.', 'Most', 'five', 'year', 'olds', 'would', 'be', 'able', 'to', 'come', 'up', 'with', 'more', 'realistic', 'dialogue', 'and', 'a', 'more', 'plausible', 'plot', '.', 'As', 'for', 'the', 'acting', 'performances', ',', 'if', 'you', 'can', 'imagine', 'the', 'most', 'rubbish', 'porno', 'you', 'have', 'ever', 'seen', '-', 'one', 'of', 'those', 'ones', 'where', 'the', 'action', 'is', 'padded', 'out', 'with', 'some', 'interminable', \"'story\", \"'\", 'to', 'explain', 'how', 'some', 'pouting', 'old', 'peroxide', 'blonde', 'boiler', 'has', 'come', 'to', 'be', 'getting', 'spit-roasted', 'by', 'a', 'couple', 'of', 'blokes', 'with', 'moustaches', '-', 'you', 'will', 'have', 'some', 'idea', 'of', 'the', 'standard', 'of', 'acting', 'in', \"'Maiden\", 'Voyage', \"'\", '.', 'Worse', 'still', ',', 'you', 'ca', \"n't\", 'even', 'fast', 'forward', 'to', 'the', 'sex', 'scenes', ',', 'because', 'there', 'are', \"n't\", 'any', '.', 'An', 'appallingly', 'dreadful', 'film', '.']\n"
     ]
    }
   ],
   "source": [
    "# define constants for the files we are using\n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "DEV_FILE = \"movie_reviews_dev.txt\"\n",
    "\n",
    "# load in your data and make sure you understand the format\n",
    "# Do not print out too much so as to impede readability of your notebook\n",
    "train_tups = sutils.generate_tuples_from_file(TRAIN_FILE)\n",
    "dev_tups = sutils.generate_tuples_from_file(DEV_FILE)\n",
    "train_tokens, train_labels = train_tups\n",
    "dev_tokens, dev_labels = dev_tups\n",
    "\n",
    "print(train_tups[0][0])\n",
    "# some variables you may want to use\n",
    "BINARIZED = False\n",
    "USE_COUNT_VECTORIZER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the functions needed (here or in sentiment_utils.py) to create vectorized BoW representations\n",
    "# of your data. We recommend starting with a multinomial BoW representation.\n",
    "# Each training example should be represented as a sparse vector.\n",
    "\n",
    "def featurize(train_list):\n",
    "    features = []\n",
    "    for toks, label in train_list:\n",
    "        if label == 1:\n",
    "            for w in toks:\n",
    "                if w not in features:\n",
    "                    features.append(w)\n",
    "    return features\n",
    "\n",
    "def vectorize(X_train, features, binarized=False):\n",
    "    # implement this function to featurize your data\n",
    "    # use nltk.word_tokenize to tokenize the emails\n",
    "    X = []\n",
    "\n",
    "    sample_count = 0\n",
    "    for toks in X_train:\n",
    "        X.append([])\n",
    "        for w in features:\n",
    "            if binarized:\n",
    "                if w in toks:\n",
    "                    X[sample_count].append(1)\n",
    "                else:\n",
    "                    X[sample_count].append(0)\n",
    "            else:\n",
    "                X[sample_count].append(toks.count(w))\n",
    "            \n",
    "        sample_count += 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That took: 700.7710444927216 seconds\n"
     ]
    }
   ],
   "source": [
    "# how much time does it take to featurize the all data with your implementation?\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "train_list = list(zip(*train_tups))\n",
    "feats = featurize(train_list)\n",
    "X = vectorize(train_tokens, feats)\n",
    "X_bin = vectorize(train_tokens, feats, binarized=True)\n",
    "\n",
    "dev_list = list(zip(*dev_tups))\n",
    "X_dev = vectorize(dev_tokens, feats)\n",
    "X_dev_bin = vectorize(dev_tokens, feats, binarized=True)\n",
    "\n",
    "end = time.time()\n",
    "print(\"That took:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20418\n"
     ]
    }
   ],
   "source": [
    "print(len(X_dev[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That took: 96.44429540634155 seconds\n"
     ]
    }
   ],
   "source": [
    "# how much time does it take to featurize the all data with sklearn's CountVectorizer?\n",
    "\n",
    "start = time.time()\n",
    "corpus = []\n",
    "for toks in train_tokens:\n",
    "    review = \"\"\n",
    "    for tok in toks:\n",
    "        review += tok +\" \"\n",
    "    corpus.append(review)\n",
    "    \n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer_bin = CountVectorizer(binary=True)\n",
    "X_vect = vectorizer.fit_transform(corpus).toarray()\n",
    "X_vect_bin = vectorizer_bin.fit_transform(corpus).toarray()\n",
    "\n",
    "X_dev_vect = vectorize(dev_tokens, vectorizer.get_feature_names_out())\n",
    "X_dev_vect_bin = vectorize(dev_tokens, vectorizer_bin.get_feature_names_out(), binarized=True)\n",
    "\n",
    "end = time.time()\n",
    "print(\"That took:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20418"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_bin[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How big is your vocabulary using your vectorization function(s)? 20418\n",
    "2. How big is your vocabulary using the `CountVectorizer`? 22596"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.993008099471055\n",
      "0.9939459417596035\n"
     ]
    }
   ],
   "source": [
    "#  write any code you need analyze the relative sparsity of your vectorized representations of the data\n",
    "\n",
    "def count_zeros(X):\n",
    "    num_zeros = 0\n",
    "    for row in X:\n",
    "        for val in row:\n",
    "            if val == 0:\n",
    "                num_zeros+=1\n",
    "    return num_zeros\n",
    "\n",
    "print(count_zeros(X)/(len(X)*len(X[0])))\n",
    "\n",
    "# Print out the average % of entries that are zeros in each vector in the vectorized training data\n",
    "print(count_zeros(X_vect)/(len(X_vect)*len(X_vect[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wendy\\anaconda3\\envs\\ds\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LogisticRegression()\n",
    "if USE_COUNT_VECTORIZER:\n",
    "    if BINARIZED:\n",
    "        model.fit(X_vect_bin, train_labels)\n",
    "        predictions = model.predict(X_dev_vect_bin)\n",
    "    else: \n",
    "        model.fit(X_vect, train_labels)\n",
    "        predictions = model.predict(X_dev_vect)\n",
    "else: \n",
    "    if BINARIZED:\n",
    "        model.fit(X_bin, train_labels)\n",
    "        predictions = model.predict(X_dev_bin)\n",
    "    else:\n",
    "        model.fit(X, train_labels)\n",
    "        predictions = model.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.scores import (precision, recall, f_measure, accuracy)\n",
    "\n",
    "\n",
    "def get_pfra(predictions, dev_labels, verbose=False):\n",
    "    if verbose:\n",
    "        print(predictions)\n",
    "        print(dev_labels)\n",
    "    # set of indicies labelled 1\n",
    "    refset_pos = set()\n",
    "    testset_pos = set()\n",
    "    for i in range(len(dev_labels)):\n",
    "        if dev_labels[i] == 1:\n",
    "            refset_pos.add(i)\n",
    "        if predictions[i] == 1:\n",
    "            testset_pos.add(i)\n",
    "    return (precision(refset_pos, testset_pos), recall(refset_pos, testset_pos), f_measure(refset_pos, testset_pos), accuracy(dev_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wendy\\anaconda3\\envs\\ds\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_measure: 0.7729468599033816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wendy\\anaconda3\\envs\\ds\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_measure: 0.7887323943661972\n"
     ]
    }
   ],
   "source": [
    "# Test the following 4 combinations to determine which has the best final f1 score for your Logistic Regression model:\n",
    "model_mult = LogisticRegression()\n",
    "model_vect_mult = LogisticRegression()\n",
    "model_bin = LogisticRegression()\n",
    "model_vect_bin = LogisticRegression()\n",
    "\n",
    "\n",
    "# your vectorized features, multinomial\n",
    "model_mult.fit(X, train_labels)\n",
    "predictions = model_mult.predict(X_dev)\n",
    "\n",
    "p, r, f, a = get_pfra(predictions, dev_labels)\n",
    "print('f_measure:', f)\n",
    "\n",
    "# CountVectorizer features, multinomial\n",
    "model_vect_mult.fit(X_vect, train_labels)\n",
    "predictions = model_vect_mult.predict(X_dev_vect)\n",
    "\n",
    "p, r, f, a = get_pfra(predictions, dev_labels)\n",
    "print('f_measure:', f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_measure: 0.8115942028985507\n",
      "f_measure: 0.8037383177570093\n"
     ]
    }
   ],
   "source": [
    "# your vectorized features, binarized\n",
    "model_bin.fit(X_bin, train_labels)\n",
    "predictions = model_bin.predict(X_dev_bin)\n",
    "\n",
    "p, r, f, a = get_pfra(predictions, dev_labels)\n",
    "print('f_measure:', f)\n",
    "\n",
    "# CountVectorizer features, binarized\n",
    "model_vect_bin.fit(X_vect_bin, train_labels)\n",
    "predictions = model_vect_bin.predict(X_dev_vect_bin)\n",
    "\n",
    "p, r, f, a = get_pfra(predictions, dev_labels)\n",
    "print('f_measure:', f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the following 4 combinations to determine which has the best final f1 score for your Logistic Regression model:\n",
    "- your vectorized features, multinomial: __0.7729468599033816__\n",
    "- CountVectorizer features, multinomial: __0.7887323943661972__\n",
    "- your vectorized features, binarized: __0.8115942028985507__\n",
    "- CountVectorizer features, binarized: __0.8037383177570093__\n",
    "\n",
    "Produce your graph(s) for the combination with the best final f1 score.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRFA Scores for 10% of training set:  (0.7578947368421053, 0.6857142857142857, 0.72, 0.72)\n",
      "PRFA Scores for 20% of training set:  (0.7959183673469388, 0.7428571428571429, 0.768472906403941, 0.765)\n",
      "PRFA Scores for 30% of training set:  (0.7889908256880734, 0.819047619047619, 0.8037383177570093, 0.79)\n",
      "PRFA Scores for 40% of training set:  (0.7837837837837838, 0.8285714285714286, 0.8055555555555556, 0.79)\n",
      "PRFA Scores for 50% of training set:  (0.8173076923076923, 0.8095238095238095, 0.8133971291866028, 0.805)\n",
      "PRFA Scores for 60% of training set:  (0.8058252427184466, 0.7904761904761904, 0.798076923076923, 0.79)\n",
      "PRFA Scores for 70% of training set:  (0.8095238095238095, 0.8095238095238095, 0.8095238095238095, 0.8)\n",
      "PRFA Scores for 80% of training set:  (0.7962962962962963, 0.819047619047619, 0.8075117370892019, 0.795)\n",
      "PRFA Scores for 90% of training set:  (0.8316831683168316, 0.8, 0.8155339805825242, 0.81)\n",
      "PRFA Scores for 100% of training set:  (0.8235294117647058, 0.8, 0.8115942028985507, 0.805)\n"
     ]
    }
   ],
   "source": [
    "# Using the provided dev set, evaluate your model with precision, recall, and f1 score as well as accuracy\n",
    "# You may use nltk's implemented `precision`, `recall`, `f_measure`, and `accuracy` functions\n",
    "# (make sure to look at the documentation for these functions!)\n",
    "# you will be creating a similar graph for logistic regression and neural nets, so make sure\n",
    "# you use functions wisely so that you do not have excessive repeated code\n",
    "# write any helper functions you need in sentiment_utils.py (functions that you'll use in your other notebooks as well)\n",
    "\n",
    "training_data = []\n",
    "for i in range(1, 11):\n",
    "    end_index = int(len(X_bin) * i / 10)\n",
    "    training_data.append((X_bin[:end_index], train_labels[:end_index]))\n",
    "\n",
    "\n",
    "eval_data = []\n",
    "\n",
    "i = 1\n",
    "for feats, labels in training_data: # for each entry in the subset.\n",
    "    model_bin.fit(feats, labels)\n",
    "    predictions = model_bin.predict(X_dev_bin)\n",
    "    \n",
    "    prfa = get_pfra(predictions, dev_labels)\n",
    "    print(f\"PRFA Scores for {i * 10}% of training set: \", prfa)\n",
    "    i += 1\n",
    "    eval_data.append(prfa)\n",
    "\n",
    "# create a graph of your classifier's performance on the dev set as a function of the amount of training data\n",
    "# the x-axis should be the amount of training data (as a percentage of the total training data)\n",
    "# NOTE : make sure one of your experiments uses 10% of the data, you will need this to answer the first question in task 5\n",
    "# the y-axis should be the performance of the classifier on the dev set\n",
    "# the graph should have 4 lines, one for each of precision, recall, f1, and accuracy\n",
    "# the graph should have a legend, title, and axis labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# x-axis: percentage of training data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# y-axis: performance of the classifier on the dev set\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 4 lines: precision, recall, f1, accuracy\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m101\u001b[39m, \u001b[38;5;241m10\u001b[39m)], [x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m eval_data], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m101\u001b[39m, \u001b[38;5;241m10\u001b[39m)], [x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m eval_data], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m101\u001b[39m, \u001b[38;5;241m10\u001b[39m)], [x[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m eval_data], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1 Score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# x-axis: percentage of training data\n",
    "# y-axis: performance of the classifier on the dev set\n",
    "# 4 lines: precision, recall, f1, accuracy\n",
    "\n",
    "plt.plot([i for i in range(10, 101, 10)], [x[0] for x in eval_data], label=\"Precision\")\n",
    "plt.plot([i for i in range(10, 101, 10)], [x[1] for x in eval_data], label=\"Recall\")\n",
    "plt.plot([i for i in range(10, 101, 10)], [x[2] for x in eval_data], label=\"F1 Score\")\n",
    "plt.plot([i for i in range(10, 101, 10)], [x[3] for x in eval_data], label=\"Accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"Percentage of Training Data\")\n",
    "plt.ylabel(\"Performance of Classifier on Dev Set\")\n",
    "plt.title(\"Classifier Metrics with Different Training Data Percentages\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
